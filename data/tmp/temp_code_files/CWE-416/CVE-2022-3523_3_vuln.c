static void migrate_vma_unmap(struct migrate_vma *migrate)
{
	const unsigned long npages = migrate->npages;
	unsigned long i, restore = 0;
	bool allow_drain = true;

	lru_add_drain();

	for (i = 0; i < npages; i++) {
		struct page *page = migrate_pfn_to_page(migrate->src[i]);
		struct folio *folio;

		if (!page)
			continue;

		/* ZONE_DEVICE pages are not on LRU */
		if (!is_zone_device_page(page)) {
			if (!PageLRU(page) && allow_drain) {
				/* Drain CPU's pagevec */
				lru_add_drain_all();
				allow_drain = false;
			}

			if (isolate_lru_page(page)) {
				migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
				migrate->cpages--;
				restore++;
				continue;
			}

			/* Drop the reference we took in collect */
			put_page(page);
		}

		folio = page_folio(page);
		if (folio_mapped(folio))
			try_to_migrate(folio, 0);

		if (page_mapped(page) || !migrate_vma_check_page(page)) {
			if (!is_zone_device_page(page)) {
				get_page(page);
				putback_lru_page(page);
			}

			migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
			migrate->cpages--;
			restore++;
			continue;
		}
	}

	for (i = 0; i < npages && restore; i++) {
		struct page *page = migrate_pfn_to_page(migrate->src[i]);
		struct folio *folio;

		if (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))
			continue;

		folio = page_folio(page);
		remove_migration_ptes(folio, folio, false);

		migrate->src[i] = 0;
		folio_unlock(folio);
		folio_put(folio);
		restore--;
	}
}