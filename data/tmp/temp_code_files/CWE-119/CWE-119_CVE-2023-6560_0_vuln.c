static void *__io_uaddr_map(struct page ***pages, unsigned short *npages,
			    unsigned long uaddr, size_t size)
{
	struct page **page_array;
	unsigned int nr_pages;
	int ret, i;

	*npages = 0;

	if (uaddr & (PAGE_SIZE - 1) || !size)
		return ERR_PTR(-EINVAL);

	nr_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
	if (nr_pages > USHRT_MAX)
		return ERR_PTR(-EINVAL);
	page_array = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);
	if (!page_array)
		return ERR_PTR(-ENOMEM);

	ret = pin_user_pages_fast(uaddr, nr_pages, FOLL_WRITE | FOLL_LONGTERM,
					page_array);
	if (ret != nr_pages) {
err:
		io_pages_free(&page_array, ret > 0 ? ret : 0);
		return ret < 0 ? ERR_PTR(ret) : ERR_PTR(-EFAULT);
	}
	/*
	 * Should be a single page. If the ring is small enough that we can
	 * use a normal page, that is fine. If we need multiple pages, then
	 * userspace should use a huge page. That's the only way to guarantee
	 * that we get contigious memory, outside of just being lucky or
	 * (currently) having low memory fragmentation.
	 */
	if (page_array[0] != page_array[ret - 1])
		goto err;

	/*
	 * Can't support mapping user allocated ring memory on 32-bit archs
	 * where it could potentially reside in highmem. Just fail those with
	 * -EINVAL, just like we did on kernels that didn't support this
	 * feature.
	 */
	for (i = 0; i < nr_pages; i++) {
		if (PageHighMem(page_array[i])) {
			ret = -EINVAL;
			goto err;
		}
	}

	*pages = page_array;
	*npages = nr_pages;
	return page_to_virt(page_array[0]);
}