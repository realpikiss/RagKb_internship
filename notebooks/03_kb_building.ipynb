{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6759d4cf",
   "metadata": {},
   "source": [
    "# KB2 Construction - Advanced Retrieval Knowledge Base with Chunked Embeddings\n",
    "\n",
    "## Academic Context\n",
    "This notebook implements an advanced KB2 (Knowledge Base 2) construction pipeline for vulnerability detection.\n",
    "Key innovations:\n",
    "- **Chunked code embeddings**: Handle long functions beyond API context limits\n",
    "- **Dual-path processing**: Normal CPG analysis vs. fallback heuristic extraction for flat CPGs\n",
    "- **Double indexation**: Separate vuln/patch vectors for improved retrieval precision\n",
    "- **Hybrid retrieval**: Dense embeddings + sparse TF-IDF for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7da10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration & Dependencies\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import gc\n",
    "import re\n",
    "import tiktoken  # For accurate token counting\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import logging\n",
    "\n",
    "# Configure logging for academic reproducibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name=='notebooks' else Path.cwd()\n",
    "CPG_JSON_DIR = PROJECT_ROOT/'data'/'tmp'/'cpg_json'\n",
    "OUTPUT_DIR = PROJECT_ROOT/'data'/'processed'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "SANITY_REPORT = OUTPUT_DIR / 'cpg_sanity_report.json'\n",
    "\n",
    "# Chunking configuration\n",
    "CHUNK_SIZE = 800  # tokens per chunk (below OpenAI's limit with buffer)\n",
    "CHUNK_OVERLAP = 100  # overlap tokens between chunks\n",
    "MAX_CHUNKS_PER_FUNCTION = 10  # limit chunks to avoid explosion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc9261",
   "metadata": {},
   "source": [
    "## Step 1: Load Prerequisites\n",
    "Load KB1 metadata and CPG sanity report to identify flat CPGs requiring fallback processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850f815e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 12:16:48,243 - INFO - Generating CPG sanity report...\n",
      "2025-08-08 12:16:48,464 - INFO - Found 4634 CPG files\n",
      "Checking CPGs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4634/4634 [01:20<00:00, 57.88it/s] \n",
      "2025-08-08 12:18:08,583 - INFO - ‚úÖ Sanity report saved: /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/processed/cpg_sanity_report.json\n",
      "2025-08-08 12:18:08,583 - INFO -    Total: 4634, Flat: 375, Parse errors: 0\n",
      "2025-08-08 12:18:08,597 - INFO - Identified 375 flat CPGs\n",
      "2025-08-08 12:18:08,597 - INFO - üìä KB1 loaded: 2317 entries\n",
      "2025-08-08 12:18:08,597 - INFO - üìä Unique CVEs: 1154\n",
      "2025-08-08 12:18:08,599 - INFO - üìä Flat CPGs: 375\n",
      "2025-08-08 12:18:08,599 - INFO - üìÑ Sample KB1 entry: CWE-119_CVE-2014-3182_0\n",
      "2025-08-08 12:18:08,600 - INFO -    ‚Ä¢ CVE: CVE-2014-3182\n",
      "2025-08-08 12:18:08,600 - INFO -    ‚Ä¢ CWE: CWE-119\n",
      "2025-08-08 12:18:08,600 - INFO -    ‚Ä¢ Desc: Invalid user input provided to the device index, w...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to KB1 metadata file\n",
    "kb1_path = PROJECT_ROOT / 'data' / 'processed' / 'kb1.json'\n",
    "kb1 = json.load(open(kb1_path, encoding='utf-8'))\n",
    "\n",
    "def build_kb1_index(data):\n",
    "    \"\"\"Build KB1 index using composite keys and CVE IDs.\"\"\"\n",
    "    kb1_by_composite = {}\n",
    "    kb1_by_cve = defaultdict(list)\n",
    "    for composite_key, entry_data in data.items():\n",
    "        kb1_by_composite[composite_key] = entry_data\n",
    "        cve_id = entry_data.get('cve_id', '')\n",
    "        if cve_id:\n",
    "            kb1_by_cve[cve_id].append({\n",
    "                'composite_key': composite_key,\n",
    "                'cwe_id': entry_data.get('cwe_id', ''),\n",
    "                'data': entry_data\n",
    "            })\n",
    "    return kb1_by_composite, kb1_by_cve\n",
    "\n",
    "kb1_by_composite, kb1_by_cve = build_kb1_index(kb1)\n",
    "\n",
    "def get_meta_from_kb1(composite_key, kb1_data):\n",
    "    \"\"\"Get KB1 metadata by composite key.\"\"\"\n",
    "    return kb1_data.get(composite_key)\n",
    "\n",
    "# --- Sanity report helpers ---\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "EXPECTED_TYPES = {\"CALL\", \"IDENTIFIER\", \"METHOD\", \"CONTROL_STRUCTURE\"}\n",
    "UNKNOWN_LABEL = \"UNKNOWN\"\n",
    "_CVE_RE = re.compile(r\"^CVE-\\d{4}-\\d+(?:_(\\d+))?$\")\n",
    "\n",
    "def _meta_from_path(p: Path):\n",
    "    \"\"\"Extract {cwe,cve,instance,kind,composite_key} from path.\"\"\"\n",
    "    parts = list(p.parts)\n",
    "    cwe = next((seg for seg in parts if seg.startswith(\"CWE-\")), None)\n",
    "    cve = None\n",
    "    inst = None\n",
    "    for seg in parts:\n",
    "        m = _CVE_RE.match(seg)\n",
    "        if m:\n",
    "            cve = seg.split(\"_\")[0]\n",
    "            if m.group(1) and m.group(1).isdigit():\n",
    "                inst = m.group(1)\n",
    "            break\n",
    "    if cve and inst is None:\n",
    "        parent = p.parent.name\n",
    "        if parent.isdigit():\n",
    "            inst = parent\n",
    "    if cve and inst is None:\n",
    "        for seg in parts:\n",
    "            if seg.startswith(cve + \"_\"):\n",
    "                tail = seg[len(cve) + 1:]\n",
    "                if tail.isdigit():\n",
    "                    inst = tail\n",
    "                    break\n",
    "    fname = p.name.lower()\n",
    "    if \"vuln\" in fname:\n",
    "        kind = \"vuln\"\n",
    "    elif \"patch\" in fname or \"safe\" in fname:\n",
    "        kind = \"patch\"\n",
    "    else:\n",
    "        kind = \"unknown\"\n",
    "    if not (cwe and cve and inst):\n",
    "        return None\n",
    "    return {\"cwe\": cwe, \"cve\": cve, \"instance\": inst, \"kind\": kind,\n",
    "            \"composite_key\": f\"{cwe}_{cve}_{inst}\"}\n",
    "\n",
    "def _import_graphson_parser():\n",
    "    \"\"\"Try importing GraphSONParser from various locations.\"\"\"\n",
    "    try:\n",
    "        from graphson_parser import GraphSONParser\n",
    "        return GraphSONParser\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        sys.path.append(str(PROJECT_ROOT / \"scripts\"))\n",
    "        from kb2_preprocessing.graphson_parser import GraphSONParser\n",
    "        return GraphSONParser\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from scripts.kb2_preprocessing.graphson_parser import GraphSONParser\n",
    "        return GraphSONParser\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _load_vertices_edges(obj):\n",
    "    \"\"\"Get vertices and edges from parser or path.\"\"\"\n",
    "    if hasattr(obj, \"parse\"):\n",
    "        try:\n",
    "            if not obj.parse():\n",
    "                return [], []\n",
    "        except Exception:\n",
    "            return [], []\n",
    "        vertices = getattr(obj, \"vertices\", []) or []\n",
    "        edges = getattr(obj, \"edges\", []) or []\n",
    "        if not vertices and hasattr(obj, \"get_vertices\"):\n",
    "            vertices = obj.get_vertices() or []\n",
    "        if not edges and hasattr(obj, \"get_edges\"):\n",
    "            edges = obj.get_edges() or []\n",
    "        return vertices, edges\n",
    "    # fallback: read raw JSON\n",
    "    try:\n",
    "        with open(obj, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = json.load(f)\n",
    "        if \"vertices\" in doc:\n",
    "            return doc.get(\"vertices\", []), doc.get(\"edges\", [])\n",
    "        val = doc.get(\"@value\", {})\n",
    "        return val.get(\"vertices\", []), val.get(\"edges\", [])\n",
    "    except Exception:\n",
    "        return [], []\n",
    "\n",
    "def generate_sanity_report():\n",
    "    \"\"\"Scan recursively for *_cpg.json and flag flat CPGs.\"\"\"\n",
    "    logger.info(\"Generating CPG sanity report...\")\n",
    "    GraphSONParser = _import_graphson_parser()\n",
    "    files = sorted(CPG_JSON_DIR.rglob(\"*_cpg.json\"))\n",
    "    logger.info(f\"Found {len(files)} CPG files\")\n",
    "    items = {}\n",
    "    flat_count = parse_errors = 0\n",
    "    for cpg_file in tqdm(files, desc=\"Checking CPGs\"):\n",
    "        meta = _meta_from_path(cpg_file)\n",
    "        if not meta:\n",
    "            continue\n",
    "        key = f\"{meta['composite_key']}::{meta['kind']}\"\n",
    "        item = {\"composite_key\": meta[\"composite_key\"], \"kind\": meta[\"kind\"],\n",
    "                \"path\": str(cpg_file), \"metrics\": {}, \"flat\": True, \"notes\": []}\n",
    "        try:\n",
    "            parser = GraphSONParser(str(cpg_file)) if GraphSONParser else None\n",
    "            vertices, edges = _load_vertices_edges(parser if parser else cpg_file)\n",
    "            vcount, ecount = len(vertices), len(edges)\n",
    "            vtypes = Counter(v.get(\"label\", UNKNOWN_LABEL) for v in vertices)\n",
    "            has_sem = bool(EXPECTED_TYPES & set(vtypes.keys()))\n",
    "            unknown_ratio = (vtypes.get(UNKNOWN_LABEL, 0) / vcount) if vcount else 1.0\n",
    "            epv = (ecount / vcount) if vcount else 0.0\n",
    "            flat = (vcount < 25) or (not has_sem) or (epv < 0.5 and vcount < 200) or (unknown_ratio > 0.6)\n",
    "            item[\"metrics\"] = {\"num_vertices\": vcount, \"num_edges\": ecount,\n",
    "                               \"edge_per_vertex\": epv, \"unknown_ratio\": unknown_ratio,\n",
    "                               \"has_semantic_nodes\": has_sem}\n",
    "            item[\"flat\"] = bool(flat)\n",
    "            if not has_sem: item[\"notes\"].append(\"missing_semantic_nodes\")\n",
    "            if unknown_ratio > 0.6: item[\"notes\"].append(\"high_unknown_ratio\")\n",
    "            if vcount < 25: item[\"notes\"].append(\"too_few_vertices\")\n",
    "            if epv < 0.5 and vcount < 200: item[\"notes\"].append(\"sparse_graph\")\n",
    "        except Exception as e:\n",
    "            item[\"notes\"].append(\"parse_failed\")\n",
    "            item[\"notes\"].append(str(e))\n",
    "            parse_errors += 1\n",
    "        if item[\"flat\"]:\n",
    "            flat_count += 1\n",
    "        items[key] = item\n",
    "    report = {\"version\": \"1.0\", \"generated_at\": datetime.now().isoformat(),\n",
    "              \"stats\": {\"total\": len(items), \"flat\": flat_count, \"parse_errors\": parse_errors},\n",
    "              \"items\": items}\n",
    "    with open(SANITY_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    logger.info(f\"‚úÖ Sanity report saved: {SANITY_REPORT}\")\n",
    "    logger.info(f\"   Total: {len(items)}, Flat: {flat_count}, Parse errors: {parse_errors}\")\n",
    "    return report\n",
    "\n",
    "def load_sanity_report():\n",
    "    \"\"\"Load sanity report, return flat index set and full items dict.\"\"\"\n",
    "    if not SANITY_REPORT.exists():\n",
    "        generate_sanity_report()\n",
    "    with open(SANITY_REPORT, \"r\", encoding=\"utf-8\") as f:\n",
    "        report = json.load(f)\n",
    "    items = report.get(\"items\", {})\n",
    "    flat_index = {k for k, v in items.items() if v.get(\"flat\")}\n",
    "    logger.info(f\"Identified {len(flat_index)} flat CPGs\")\n",
    "    return flat_index, items\n",
    "\n",
    "# Load report\n",
    "flat_cpgs, sanity_items = load_sanity_report()\n",
    "\n",
    "# KB1 stats\n",
    "logger.info(f\"üìä KB1 loaded: {len(kb1_by_composite)} entries\")\n",
    "logger.info(f\"üìä Unique CVEs: {len(kb1_by_cve)}\")\n",
    "logger.info(f\"üìä Flat CPGs: {len(flat_cpgs)}\")\n",
    "\n",
    "if kb1_by_composite:\n",
    "    sample_key = next(iter(kb1_by_composite))\n",
    "    sample_entry = kb1_by_composite[sample_key]\n",
    "    logger.info(f\"üìÑ Sample KB1 entry: {sample_key}\")\n",
    "    logger.info(f\"   ‚Ä¢ CVE: {sample_entry.get('cve_id', 'N/A')}\")\n",
    "    logger.info(f\"   ‚Ä¢ CWE: {sample_entry.get('cwe_id', 'N/A')}\")\n",
    "    logger.info(f\"   ‚Ä¢ Desc: {sample_entry.get('vulnerability_type', 'N/A')[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ec936",
   "metadata": {},
   "source": [
    "## Step 2: Code Chunking Strategy\n",
    "using  intelligent code chunking to handle long functions beyond embedding API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d9e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 12:21:43,058 - INFO - Test chunking: 29 chunks from 78 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from kb2_preprocessing.chunking import (\n",
    "    count_tokens,             # alias vers count_tokens du module\n",
    "    find_logical_boundaries,  # idem\n",
    "    chunk_code,               # idem\n",
    ")\n",
    "\n",
    "# Test chunking sur un exemple\n",
    "test_code = \"\"\"void vulnerable_function(char *input) {\n",
    "    char buffer[100];\n",
    "    strcpy(buffer, input);  // Buffer overflow vulnerability\n",
    "    \n",
    "    if (strlen(input) > 50) {\n",
    "        printf(\"Input too long\\n\");\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        process_data(buffer, i);\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "test_chunks = chunk_code(test_code, max_tokens=50)  # chunks courts pour tester\n",
    "logger.info(f\"Test chunking: {len(test_chunks)} chunks from {count_tokens(test_code)} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5feac",
   "metadata": {},
   "source": [
    "## Step 3: Find and Process Vulnerability Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad083e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 12:22:22,896 - INFO - Successfully imported kb2_preprocessing modules\n",
      "2025-08-08 12:22:22,939 - INFO - Found 2317 vulnerability/patch pairs\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Find Vulnerability Pairs\n",
    "\n",
    "# Add kb2_preprocessing to path\n",
    "sys.path.append(str(PROJECT_ROOT / 'scripts'))\n",
    "try:\n",
    "    from kb2_preprocessing import GraphSONParser, extract_kb2_features\n",
    "    logger.info(\"Successfully imported kb2_preprocessing modules\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"Failed to import kb2_preprocessing: {e}\")\n",
    "    GraphSONParser = None\n",
    "    extract_kb2_features = None\n",
    "\n",
    "def find_pairs(dir_):\n",
    "    \"\"\"\n",
    "    Find all vulnerability/patch pairs in the CPG directory using the correct composite keys.\n",
    "    This function iterates over CWE directories and their instances, looking for both\n",
    "    'vuln_cpg.json' and 'patch_cpg.json' files. If both files exist, it constructs a composite key\n",
    "    (matching the KB1 format) and stores relevant metadata for each pair.\n",
    "    \"\"\"\n",
    "    pairs = {}\n",
    "    for cwe_dir in dir_.iterdir():\n",
    "        # Only process directories whose names start with 'CWE-'\n",
    "        if not cwe_dir.name.startswith('CWE-'):\n",
    "            continue\n",
    "        for inst in cwe_dir.iterdir():\n",
    "            vuln_file = inst / 'vuln_cpg.json'\n",
    "            patch_file = inst / 'patch_cpg.json'\n",
    "            # Check that both the vulnerable and patch CPG files exist\n",
    "            if vuln_file.exists() and patch_file.exists():\n",
    "                cve = extract_cve_from_path(vuln_file)\n",
    "                # Extract the instance number from the directory name\n",
    "                num = inst.name.split('_')[-1]\n",
    "                # Construct the composite key in the same format as KB1\n",
    "                composite_key = f\"{cwe_dir.name}_{cve}_{num}\"\n",
    "                pairs[composite_key] = {\n",
    "                    'vuln': vuln_file,\n",
    "                    'safe': patch_file,\n",
    "                    'cve': cve,\n",
    "                    'cwe': cwe_dir.name,\n",
    "                    'instance_id': num,\n",
    "                    'composite_key': composite_key  # Composite key for cross-referencing\n",
    "                }\n",
    "    return pairs\n",
    "\n",
    "CPG_DIR = PROJECT_ROOT / 'data' / 'tmp' / 'cpg_json'\n",
    "pairs = find_pairs(CPG_DIR)\n",
    "logger.info(f\"Found {len(pairs)} vulnerability/patch pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b710e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 12:22:49,862 - INFO - Loaded CWE patterns for 9 CWE types\n",
      "2025-08-08 12:22:49,863 - INFO - Risk weights: {'CWE-416': 0.9, 'CWE-476': 0.8, 'CWE-362': 0.7, 'CWE-119/787': 0.9, 'CWE-20': 0.6, 'CWE-200': 0.4, 'CWE-125': 0.7, 'CWE-264': 0.5, 'CWE-401': 0.6}\n",
      "2025-08-08 12:22:49,869 - INFO - Extracted features from /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/cpg_json/CWE-264/CVE-2016-6786_6/vuln_cpg.json\n",
      "2025-08-08 12:22:49,869 - INFO - Structural features: 3\n",
      "2025-08-08 12:22:49,869 - INFO - Semantic features: 5\n",
      "2025-08-08 12:22:49,875 - INFO - Loaded CWE patterns for 9 CWE types\n",
      "2025-08-08 12:22:49,876 - INFO - Risk weights: {'CWE-416': 0.9, 'CWE-476': 0.8, 'CWE-362': 0.7, 'CWE-119/787': 0.9, 'CWE-20': 0.6, 'CWE-200': 0.4, 'CWE-125': 0.7, 'CWE-264': 0.5, 'CWE-401': 0.6}\n",
      "2025-08-08 12:22:49,879 - INFO - Extracted features from /Users/vernetemmanueladjobi/Documents/RessourcesStages/Projets/VulRAG-Hybrid-System/data/tmp/cpg_json/CWE-264/CVE-2016-6786_6/patch_cpg.json\n",
      "2025-08-08 12:22:49,879 - INFO - Structural features: 3\n",
      "2025-08-08 12:22:49,880 - INFO - Semantic features: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample extraction - Quality: 1.00\n",
      "Structural features: 12 dimensions\n",
      "Semantic features: 7 elements\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Import Feature Extraction\n",
    "import sys\n",
    "sys.path.append(str(PROJECT_ROOT/'scripts'))\n",
    "from kb2_preprocessing import extract_kb2_features\n",
    "\n",
    "# Test feature extraction\n",
    "sample_pair = next(iter(pairs.items()))\n",
    "sample_key, sample_info = sample_pair\n",
    "vf = extract_kb2_features(sample_info['vuln'])\n",
    "sf = extract_kb2_features(sample_info['safe'])\n",
    "\n",
    "print(f\"Sample extraction - Quality: {vf['extraction_metadata']['feature_completeness']['quality_score']:.2f}\")\n",
    "print(f\"Structural features: {len(vf['structural_features'])} dimensions\")\n",
    "print(f\"Semantic features: {len(vf['semantic_features'])} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75164459",
   "metadata": {},
   "source": [
    "## Step 4: Fallback Feature Extraction for Flat CPGs\n",
    "Implement heuristic feature extraction for cases where CPG parsing fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870d97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample VULN code length: 445 chars\n",
      "Sample PATCH code length: 395 chars\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Fallback Feature Extraction\n",
    "\n",
    "def extract_fallback_features(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract heuristic features from raw code when CPG is flat/missing.\n",
    "    \n",
    "    When Joern fails to parse (flat CPG), we fall back to\n",
    "    text-based and pattern-matching features to maintain coverage.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Text statistics\n",
    "    lines = code.split('\\n')\n",
    "    features['loc'] = len(lines)\n",
    "    features['avg_line_length'] = np.mean([len(l) for l in lines]) if lines else 0\n",
    "    \n",
    "    # Comment ratio\n",
    "    comment_lines = sum(1 for l in lines if l.strip().startswith('//') or '/*' in l)\n",
    "    features['comment_ratio'] = comment_lines / len(lines) if lines else 0\n",
    "    \n",
    "    # Keyword counts (control flow complexity proxy)\n",
    "    keywords = ['if', 'else', 'for', 'while', 'switch', 'case', 'return', 'goto', 'break', 'continue']\n",
    "    for kw in keywords:\n",
    "        features[f'kw_{kw}'] = len(re.findall(rf'\\b{kw}\\b', code))\n",
    "    \n",
    "    # Approximate cyclomatic complexity\n",
    "    features['approx_cyclomatic'] = 1 + sum([\n",
    "        features.get(f'kw_{kw}', 0) \n",
    "        for kw in ['if', 'for', 'while', 'case']\n",
    "    ])\n",
    "    \n",
    "    # Dangerous function calls (CWE-relevant patterns)\n",
    "    dangerous_funcs = [\n",
    "        'strcpy', 'strcat', 'sprintf', 'gets', 'scanf',  # Buffer overflow risks\n",
    "        'malloc', 'free', 'realloc',  # Memory management\n",
    "        'memcpy', 'memmove', 'memset',  # Memory operations\n",
    "        'system', 'exec', 'popen',  # Command injection risks\n",
    "    ]\n",
    "    \n",
    "    for func in dangerous_funcs:\n",
    "        features[f'call_{func}'] = len(re.findall(rf'\\b{func}\\s*\\(', code))\n",
    "    \n",
    "    # Pointer/memory access patterns\n",
    "    features['ptr_deref'] = len(re.findall(r'\\*\\w+', code))\n",
    "    features['field_access'] = len(re.findall(r'\\w+->\\w+', code))\n",
    "    features['array_access'] = len(re.findall(r'\\w+\\[.*?\\]', code))\n",
    "    \n",
    "    # Null check patterns (safety indicators)\n",
    "    features['null_checks'] = len(re.findall(r'if\\s*\\(!?\\s*\\w+\\s*[!=]=\\s*NULL', code, re.IGNORECASE))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_code_from_cpg(cpg_path: Path) -> Optional[str]:\n",
    "    \"\"\"Extract raw code from CPG UNKNOWN nodes when semantic parsing failed.\"\"\"\n",
    "    if not cpg_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        parser = GraphSONParser(str(cpg_path))\n",
    "        if not parser.parse():\n",
    "            return None\n",
    "        \n",
    "        # Try to get source code from UNKNOWN nodes\n",
    "        code_parts = []\n",
    "        for vertex in parser.vertices:\n",
    "            if vertex.get('label') == 'UNKNOWN':\n",
    "                props = parser.extract_vertex_properties(vertex)\n",
    "                if 'CODE' in props:\n",
    "                    code_parts.append(props['CODE'])\n",
    "        \n",
    "        return '\\n'.join(code_parts) if code_parts else None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to extract code from CPG: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_code_body(composite_key, kb1_meta):\n",
    "    \"\"\"Load code body directly from temp_code_files\"\"\"\n",
    "    # Parse composite key: CWE-416_CVE-2014-3182_1\n",
    "    parts = composite_key.split('_')\n",
    "    if len(parts) < 3:\n",
    "        return None, None\n",
    "    \n",
    "    cwe = parts[0]  # CWE-416\n",
    "    cve = parts[1]  # CVE-2014-3182\n",
    "    instance = parts[2]  # 1\n",
    "    \n",
    "    # Build file paths with correct format\n",
    "    temp_dir = PROJECT_ROOT/'data'/'tmp'/'temp_code_files'\n",
    "    vuln_file = temp_dir/cwe/f\"{cve}_{instance}_vuln.c\"\n",
    "    patch_file = temp_dir/cwe/f\"{cve}_{instance}_patch.c\"\n",
    "    \n",
    "    # Load code\n",
    "    vuln_code = vuln_file.read_text() if vuln_file.exists() else None\n",
    "    patch_code = patch_file.read_text() if patch_file.exists() else None\n",
    "    \n",
    "    return vuln_code, patch_code\n",
    "\n",
    "sample_key = list(pairs.keys())[0]\n",
    "sample_info = pairs[sample_key]\n",
    "sample_kb1 = get_meta_from_kb1(sample_key, kb1_by_composite)\n",
    "vuln_code, patch_code = load_code_body(sample_key, sample_kb1)\n",
    "\n",
    "print(f\"Sample VULN code length: {len(vuln_code) if vuln_code else 0} chars\")\n",
    "print(f\"Sample PATCH code length: {len(patch_code) if patch_code else 0} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3acba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: OpenAI Embeddings Setup\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize OpenAI client\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"OPENAI_API_KEY not found. Embeddings will be skipped.\")\n",
    "    client = None\n",
    "else:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0a50b",
   "metadata": {},
   "source": [
    "## Step 5: Chunked Embedding Generation\n",
    "Generate embeddings for code chunks with fallback strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bc154",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 6: Embedding Generation with Chunking\n",
    "\n",
    "import openai\n",
    "\n",
    "# Configure OpenAI API\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if OPENAI_API_KEY:\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    logger.info(\"OpenAI API key configured\")\n",
    "else:\n",
    "    logger.warning(\"No OpenAI API key found - embeddings will be skipped\")\n",
    "\n",
    "def generate_embedding(code: str, use_chunking: bool = True) -> Optional[np.ndarray]:\n",
    "    \"\"\"Generate embedding for code, with chunking for long functions.\n",
    "    \n",
    "    Academic innovation: Chunk-based embedding with max-pooling aggregation\n",
    "    to handle functions beyond API context limits.\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return None\n",
    "    \n",
    "    if not code or not code.strip():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Check if chunking is needed\n",
    "        token_count = count_tokens(code)\n",
    "        \n",
    "        if not use_chunking or token_count <= CHUNK_SIZE:\n",
    "            # Single embedding for short code\n",
    "            response = openai.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=code\n",
    "            )\n",
    "            embedding = np.array(response['data'][0]['embedding'])\n",
    "            return embedding\n",
    "        \n",
    "        # Chunk the code\n",
    "        chunks = chunk_code(code)\n",
    "        if not chunks:\n",
    "            return None\n",
    "        \n",
    "        # Generate embeddings for each chunk\n",
    "        chunk_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                response = openai.embeddings.create(\n",
    "                    model=\"text-embedding-3-large\",\n",
    "                    input=chunk['text']\n",
    "                )\n",
    "                chunk_emb = np.array(response['data'][0]['embedding'])\n",
    "                chunk_embeddings.append(chunk_emb)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to embed chunk {chunk['chunk_id']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not chunk_embeddings:\n",
    "            return None\n",
    "        \n",
    "        # Aggregate chunk embeddings\n",
    "        # Strategy: Max-pooling (captures most salient features across chunks)\n",
    "        chunk_matrix = np.vstack(chunk_embeddings)\n",
    "        aggregated = np.max(chunk_matrix, axis=0)\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        norm = np.linalg.norm(aggregated)\n",
    "        if norm > 0:\n",
    "            aggregated = aggregated / norm\n",
    "        \n",
    "        logger.info(f\"Generated chunked embedding from {len(chunk_embeddings)} chunks\")\n",
    "        return aggregated\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Embedding generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_chunk_embeddings(code: str) -> Tuple[Optional[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"Generate both aggregated and individual chunk embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        - aggregated: Single vector representing entire function\n",
    "        - chunk_vectors: List of vectors for each chunk (for fine-grained retrieval)\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY or not code:\n",
    "        return None, []\n",
    "    \n",
    "    chunks = chunk_code(code)\n",
    "    chunk_vectors = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = openai.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=chunk['text']\n",
    "            )\n",
    "            emb = np.array(response['data'][0]['embedding'])\n",
    "            chunk_vectors.append(emb)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Chunk embedding failed: {e}\")\n",
    "            # Add zero vector to maintain alignment\n",
    "            if chunk_vectors:\n",
    "                chunk_vectors.append(np.zeros_like(chunk_vectors[0]))\n",
    "    \n",
    "    if not chunk_vectors:\n",
    "        return None, []\n",
    "    \n",
    "    # Aggregate using max-pooling\n",
    "    aggregated = np.max(np.vstack(chunk_vectors), axis=0)\n",
    "    aggregated = aggregated / np.linalg.norm(aggregated)\n",
    "    \n",
    "    return aggregated, chunk_vectors\n",
    "\n",
    "def generate_embedding_old(code):\n",
    "    \"\"\"Generate OpenAI embedding for code snippet\"\"\"\n",
    "    if not client or not code:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=code\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019619e",
   "metadata": {},
   "source": [
    "## Step 6: KB2 Construction Pipeline\n",
    "Main pipeline integrating CPG analysis, fallback extraction, and chunked embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf87ab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 7: Build KB2 with Advanced Features\n",
    "\n",
    "def process_single_entry(composite_key: str, kind: str, cpg_path: Path, kb1_meta: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single KB2 entry with fallback handling.\n",
    "    \n",
    "    Academic contribution: Dual-path processing based on CPG quality.\n",
    "    \"\"\"\n",
    "    entry = {\n",
    "        'composite_key': composite_key,\n",
    "        'kind': kind,\n",
    "        'cpg_path': str(cpg_path),\n",
    "        'mode': 'normal',  # normal or fallback\n",
    "        'flat_cpg': False,\n",
    "        'features': {},\n",
    "        'code': '',\n",
    "        'embedding': None,\n",
    "        'chunk_embeddings': [],\n",
    "        'embedding_failed': False\n",
    "    }\n",
    "    \n",
    "    # Check if CPG is flat\n",
    "    cpg_key = f\"{composite_key}::{kind}\"\n",
    "    is_flat = cpg_key in flat_cpgs\n",
    "    \n",
    "    if is_flat:\n",
    "        logger.info(f\"Processing flat CPG: {cpg_key}\")\n",
    "        entry['flat_cpg'] = True\n",
    "        entry['mode'] = 'fallback'\n",
    "        \n",
    "        # Try to extract code from CPG UNKNOWN nodes\n",
    "        code = extract_code_from_cpg(cpg_path)\n",
    "        if not code:\n",
    "            # Load from KB1 if available\n",
    "            code_vuln, code_patch = load_code_body(composite_key, kb1_meta)\n",
    "            code = code_vuln if kind == 'vuln' else code_patch\n",
    "        \n",
    "        if code:\n",
    "            entry['code'] = code\n",
    "            entry['features'] = extract_fallback_features(code)\n",
    "        else:\n",
    "            logger.warning(f\"No code available for {cpg_key}\")\n",
    "    else:\n",
    "        # Normal CPG processing\n",
    "        try:\n",
    "            if extract_kb2_features:\n",
    "                entry['features'] = extract_kb2_features(str(cpg_path))\n",
    "                entry['mode'] = 'normal'\n",
    "            else:\n",
    "                raise ImportError(\"kb2_preprocessing not available\")\n",
    "            \n",
    "            # Get code for embedding\n",
    "            code = entry['features'].get('code_snippet', '')\n",
    "            if not code:\n",
    "                code = extract_code_from_cpg(cpg_path)\n",
    "            if not code:\n",
    "                code_vuln, code_patch = load_code_body(composite_key, kb1_meta)\n",
    "                code = code_vuln if kind == 'vuln' else code_patch\n",
    "            \n",
    "            entry['code'] = code or ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"CPG processing failed for {cpg_key}: {e}, using fallback\")\n",
    "            entry['mode'] = 'fallback'\n",
    "            entry['flat_cpg'] = True\n",
    "            \n",
    "            # Fallback to heuristic extraction\n",
    "            code = extract_code_from_cpg(cpg_path)\n",
    "            if not code:\n",
    "                code_vuln, code_patch = load_code_body(composite_key, kb1_meta)\n",
    "                code = code_vuln if kind == 'vuln' else code_patch\n",
    "            \n",
    "            if code:\n",
    "                entry['code'] = code\n",
    "                entry['features'] = extract_fallback_features(code)\n",
    "    \n",
    "    # Generate embeddings (with chunking if needed)\n",
    "    if entry['code']:\n",
    "        aggregated, chunks = generate_chunk_embeddings(entry['code'])\n",
    "        if aggregated is not None:\n",
    "            entry['embedding'] = aggregated\n",
    "            entry['chunk_embeddings'] = chunks\n",
    "        else:\n",
    "            entry['embedding_failed'] = True\n",
    "    \n",
    "    return entry\n",
    "\n",
    "def build_kb2_(pairs, limit=None):\n",
    "    \"\"\"\n",
    "    Build KB2 with advanced features:\n",
    "    - Dual-path processing (normal CPG vs fallback)\n",
    "    - Chunked embeddings for long functions\n",
    "    - Double indexation (vuln vs patch)\n",
    "    - Hybrid retrieval preparation (dense + sparse)\n",
    "    \"\"\"\n",
    "    kb2_entries = []\n",
    "\n",
    "    # Double embeddings: separate lists for vulnerable and patched code\n",
    "    vuln_embeddings = []\n",
    "    patch_embeddings = []\n",
    "    \n",
    "    # Chunk embeddings for fine-grained retrieval\n",
    "    vuln_chunk_embeddings = []\n",
    "    patch_chunk_embeddings = []\n",
    "\n",
    "    # Double structural features: separate lists for vulnerable and patched code\n",
    "    vuln_structural_features = []\n",
    "    patch_structural_features = []\n",
    "\n",
    "    # Double TF-IDF: separate lists for vulnerable and patched code\n",
    "    vuln_tfidf_texts = []\n",
    "    patch_tfidf_texts = []\n",
    "\n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'normal': 0,\n",
    "        'fallback': 0,\n",
    "        'embedding_failed': 0,\n",
    "        'chunked': 0\n",
    "    }\n",
    "\n",
    "    # Limit the number of items if a limit is specified\n",
    "    items = list(pairs.items())[:limit] if limit else pairs.items()\n",
    "\n",
    "    print(f\"Building KB2 with DOUBLE INDEXATION for {len(items)} pairs...\")\n",
    "\n",
    "    for i, (composite_key, info) in enumerate(tqdm(items, desc=\"Building KB2\")):\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "        \n",
    "        stats['total'] += 1\n",
    "\n",
    "        # Process vulnerability entry\n",
    "        vuln_entry = process_single_entry(\n",
    "            composite_key, 'vuln', info['vuln'], get_meta_from_kb1(composite_key, kb1_by_composite)\n",
    "        )\n",
    "        \n",
    "        # Process patch entry\n",
    "        patch_entry = process_single_entry(\n",
    "            composite_key, 'patch', info['patch'], get_meta_from_kb1(composite_key, kb1_by_composite)\n",
    "        )\n",
    "        \n",
    "        # Update statistics\n",
    "        for entry in [vuln_entry, patch_entry]:\n",
    "            if entry['mode'] == 'normal':\n",
    "                stats['normal'] += 1\n",
    "            else:\n",
    "                stats['fallback'] += 1\n",
    "            \n",
    "            if entry['embedding_failed']:\n",
    "                stats['embedding_failed'] += 1\n",
    "            \n",
    "            if len(entry.get('chunk_embeddings', [])) > 1:\n",
    "                stats['chunked'] += 1\n",
    "        \n",
    "        # Collect embeddings and features\n",
    "        if vuln_entry['embedding'] is not None:\n",
    "            vuln_embeddings.append(vuln_entry['embedding'])\n",
    "            vuln_chunk_embeddings.append(vuln_entry['chunk_embeddings'])\n",
    "        \n",
    "        if patch_entry['embedding'] is not None:\n",
    "            patch_embeddings.append(patch_entry['embedding'])\n",
    "            patch_chunk_embeddings.append(patch_entry['chunk_embeddings'])\n",
    "\n",
    "        # Construct the KB2 entry with all relevant metadata and features\n",
    "        entry = {\n",
    "            'composite_key': composite_key,  # Composite key from KB1\n",
    "            'cve': info['cve'],\n",
    "            'cwe': info['cwe'],\n",
    "            'instance_id': info['instance_id'],\n",
    "            'kb1_metadata': get_meta_from_kb1(composite_key, kb1_by_composite),  # Complete KB1 metadata\n",
    "\n",
    "            # Separate features for VULN and PATCH\n",
    "            'vuln_features': {\n",
    "                'structural_features': vuln_entry['features'],\n",
    "                'semantic_features': vuln_entry['features'],\n",
    "                'code_before_change': vuln_entry.get('code', '')\n",
    "            },\n",
    "            'patch_features': {\n",
    "                'structural_features': patch_entry['features'],\n",
    "                'semantic_features': patch_entry['features'],\n",
    "                'code_after_change': patch_entry.get('code', '')\n",
    "            },\n",
    "\n",
    "            # Separate embedding keys for VULN and PATCH\n",
    "            'vuln_embed_key': len(vuln_embeddings) - 1 if vuln_entry['embedding'] is not None else None,\n",
    "            'patch_embed_key': len(patch_embeddings) - 1 if patch_entry['embedding'] is not None else None,\n",
    "\n",
    "            # Timestamp for entry creation\n",
    "            'ts': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Store the vulnerable code's embedding, structural features, and TF-IDF text if available\n",
    "        if vuln_entry['embedding'] is not None:\n",
    "            vuln_structural_features.append(list(vuln_entry['features'].values()) if isinstance(vuln_entry['features'], dict) else [])\n",
    "            vuln_tfidf_texts.append(vuln_entry.get('code', ''))\n",
    "\n",
    "        # Store the patched code's embedding, structural features, and TF-IDF text if available\n",
    "        if patch_entry['embedding'] is not None:\n",
    "            patch_structural_features.append(list(patch_entry['features'].values()) if isinstance(patch_entry['features'], dict) else [])\n",
    "            patch_tfidf_texts.append(patch_entry.get('code', ''))\n",
    "\n",
    "        # Add the entry to the KB2 entries list\n",
    "        kb2_entries.append(entry)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\nüìä KB2 Build Statistics:\")\n",
    "    print(f\"  Total pairs processed: {stats['total']}\")\n",
    "    print(f\"  Normal CPG processing: {stats['normal']}\")\n",
    "    print(f\"  Fallback processing: {stats['fallback']}\")\n",
    "    print(f\"  Embedding failures: {stats['embedding_failed']}\")\n",
    "    print(f\"  Chunked embeddings: {stats['chunked']}\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    vuln_embeddings_array = np.array(vuln_embeddings, dtype='float32')\n",
    "    patch_embeddings_array = np.array(patch_embeddings, dtype='float32')\n",
    "    \n",
    "    np.save(OUTPUT_DIR/'kb2_vuln_embeddings.npy', vuln_embeddings_array)\n",
    "    np.save(OUTPUT_DIR/'kb2_patch_embeddings.npy', patch_embeddings_array)\n",
    "    \n",
    "    # Return all constructed data structures for further processing or saving\n",
    "    return (kb2_entries, vuln_embeddings, patch_embeddings, \n",
    "            vuln_chunk_embeddings, patch_chunk_embeddings,\n",
    "            vuln_structural_features, patch_structural_features, \n",
    "            vuln_tfidf_texts, patch_tfidf_texts, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acc882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion and documentation of the 4 entries with excessively large files\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# The 4 problematic entries to delete\n",
    "problematic_entries = [\n",
    "    'CWE-787_CVE-2018-10882_0',\n",
    "    'CWE-125_CVE-2016-10208_0', \n",
    "    'CWE-416_CVE-2022-1973_0',\n",
    "    'CWE-476_CVE-2018-1094_0'\n",
    "]\n",
    "\n",
    "print(\"üßπ DELETION OF PROBLEMATIC ENTRIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load existing KB2\n",
    "with open(OUTPUT_DIR/'kb2.json', 'r') as f:\n",
    "    kb2 = json.load(f)\n",
    "\n",
    "print(f\"üìä Initial state: {len(kb2)} entries in KB2\")\n",
    "\n",
    "# Document removed entries\n",
    "removed_entries_doc = {\n",
    "    'removal_timestamp': datetime.now().isoformat(),\n",
    "    'removal_reason': 'Code files too large for OpenAI embedding API (>8192 tokens)',\n",
    "    'removed_entries': {},\n",
    "    'statistics': {\n",
    "        'original_kb2_size': len(kb2),\n",
    "        'removed_count': 0,\n",
    "        'final_kb2_size': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Remove and document each entry\n",
    "removed_count = 0\n",
    "for composite_key in problematic_entries:\n",
    "    if composite_key in kb2:\n",
    "        # Save metadata before deletion\n",
    "        entry = kb2[composite_key]\n",
    "        removed_entries_doc['removed_entries'][composite_key] = {\n",
    "            'cve': entry.get('cve'),\n",
    "            'cwe': entry.get('cwe'),\n",
    "            'instance_id': entry.get('instance_id'),\n",
    "            'removal_reason': f\"Code file size too large for embedding generation\",\n",
    "            'kb1_metadata_available': entry.get('kb1_metadata') is not None,\n",
    "            'structural_features_available': entry.get('vuln_features') is not None\n",
    "        }\n",
    "        \n",
    "        # Delete the entry\n",
    "        del kb2[composite_key]\n",
    "        removed_count += 1\n",
    "        print(f\"üóëÔ∏è  Removed: {composite_key}\")\n",
    "        \n",
    "        # Display details\n",
    "        if entry.get('kb1_metadata'):\n",
    "            kb1_meta = entry['kb1_metadata']\n",
    "            print(f\"   ‚Ä¢ CVE: {entry.get('cve')}\")\n",
    "            print(f\"   ‚Ä¢ CWE: {entry.get('cwe')}\")\n",
    "            print(f\"   ‚Ä¢ Description: {kb1_meta.get('N/A')}...\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Entry {composite_key} already absent from KB2\")\n",
    "\n",
    "# Update statistics\n",
    "removed_entries_doc['statistics']['removed_count'] = removed_count\n",
    "removed_entries_doc['statistics']['final_kb2_size'] = len(kb2)\n",
    "\n",
    "print(f\"\\nüìà Results:\")\n",
    "print(f\"  Entries removed: {removed_count}\")\n",
    "print(f\"  Final KB2 size: {len(kb2)} entries\")\n",
    "print(f\"  Coverage: {len(kb2)}/{removed_entries_doc['statistics']['original_kb2_size']} = {(len(kb2)/removed_entries_doc['statistics']['original_kb2_size']*100):.1f}%\")\n",
    "\n",
    "# Save cleaned KB2\n",
    "with open(OUTPUT_DIR/'kb2.json', 'w') as f:\n",
    "    json.dump(kb2, f, indent=2)\n",
    "print(f\"\\n‚úÖ Cleaned KB2 saved: {len(kb2)} entries\")\n",
    "\n",
    "# Save removal documentation\n",
    "with open(OUTPUT_DIR/'kb2_removed_entries_log.json', 'w') as f:\n",
    "    json.dump(removed_entries_doc, f, indent=2)\n",
    "print(f\"‚úÖ Removal log saved: kb2_removed_entries_log.json\")\n",
    "\n",
    "# Display removal report\n",
    "print(f\"\\nüìã REMOVAL REPORT:\")\n",
    "print(f\"=\" * 30)\n",
    "for composite_key, details in removed_entries_doc['removed_entries'].items():\n",
    "    print(f\"‚Ä¢ {composite_key}\")\n",
    "    print(f\"  ‚îî‚îÄ {details['cwe']} / {details['cve']}\")\n",
    "    print(f\"  ‚îî‚îÄ Reason: Code file too large (>8192 tokens)\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL RESULT:\")\n",
    "print(f\"üìä Final KB2: {len(kb2)} entries (99.83% coverage)\")\n",
    "print(f\"üìù Documentation: The 4 removed entries are documented\")\n",
    "print(f\"üî• Base ready for hybrid RRF system!\")\n",
    "print(f\"üí° Negligible impact: 0.17% fewer entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03260af",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "## üìã Analyse des Fichiers KB2 Preprocessing\n",
    "\n",
    "√âvaluation de l'utilit√© et de la validit√© de tous les fichiers dans `scripts/kb2_preprocessing/` pour la reconstruction de KB2."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
