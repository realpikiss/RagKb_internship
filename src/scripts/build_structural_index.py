#!/usr/bin/env python3
"""
Build FAISS index for structural pre-filtering from vulnerability signatures.

This script reads the CSV file generated by extract_signatures.py and creates
a FAISS index for efficient similarity search based on structural features.

UPDATED: Uses unified signature extraction module for consistent feature ordering.

Usage:
    python scripts/build_structural_index.py
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path

# Add project root to path for imports
sys.path.append(str(Path(__file__).parent.parent))

try:
    import faiss
except ImportError:
    print("Error: FAISS not installed. Please install with: pip install faiss-cpu")
    sys.exit(1)

# Import unified signature extraction for consistent feature ordering
from utils.signature_extraction import get_feature_columns


def build_structural_index(csv_path: str, output_path: str):
    """
    Build FAISS index from structural signatures CSV.
    
    Args:
        csv_path: Path to the vulnerability signatures CSV file
        output_path: Path to save the FAISS index
    """
    
    print(f"Loading signatures from: {csv_path}")
    
    # Read the CSV file
    try:
        df = pd.read_csv(csv_path)
        print(f"Loaded {len(df)} signatures from CSV")
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return False
    
    # Use unified feature column definition (excludes instance_id)
    unified_feature_columns = [col for col in get_feature_columns() if col != 'instance_id']
    print(f"Using unified feature columns: {len(unified_feature_columns)} features")
    
    # Verify all columns exist in CSV
    missing_columns = [col for col in unified_feature_columns if col not in df.columns]
    if missing_columns:
        print(f"Error: Missing columns in CSV: {missing_columns}")
        print(f"Available columns: {list(df.columns)}")
        return False
    
    # Check for extra columns in CSV
    csv_feature_columns = [col for col in df.columns if col != 'instance_id']
    extra_columns = [col for col in csv_feature_columns if col not in unified_feature_columns]
    if extra_columns:
        print(f"Warning: Extra columns in CSV (will be ignored): {extra_columns}")
    
    # Extract feature matrix in EXACT unified order
    print(f"Extracting feature matrix with unified column order...")
    X = df[unified_feature_columns].values
    
    # Convert to float32 for FAISS
    X = X.astype('float32')
    
    print(f"Feature matrix shape: {X.shape}")
    print(f"Feature matrix dtype: {X.dtype}")
    
    # Verify no NaN or infinite values
    if np.any(np.isnan(X)):
        print("Warning: NaN values detected in feature matrix")
        nan_count = np.sum(np.isnan(X))
        print(f"  NaN count: {nan_count}")
        # Replace NaN with 0
        X = np.nan_to_num(X, nan=0.0)
        print("  NaN values replaced with 0.0")
    
    if np.any(np.isinf(X)):
        print("Warning: Infinite values detected in feature matrix")
        inf_count = np.sum(np.isinf(X))
        print(f"  Infinite count: {inf_count}")
        # Replace inf with large finite values
        X = np.nan_to_num(X, posinf=1e6, neginf=-1e6)
        print("  Infinite values replaced with finite values")
    
    # Feature statistics
    print(f"\nðŸ“Š Feature Statistics:")
    print(f"  Mean values: min={X.mean(axis=0).min():.3f}, max={X.mean(axis=0).max():.3f}")
    print(f"  Std values: min={X.std(axis=0).min():.3f}, max={X.std(axis=0).max():.3f}")
    print(f"  Zero features: {np.sum(X.sum(axis=0) == 0)} out of {X.shape[1]}")
    
    # Initialize FAISS index
    d = X.shape[1]  # Dimension
    print(f"\nðŸ—ï¸ Building FAISS index with dimension: {d}")
    
    # Use L2 (Euclidean) distance for structural similarity
    index = faiss.IndexFlatL2(d)
    
    # Add vectors to index
    index.add(X)
    
    print(f"Added {index.ntotal} vectors to FAISS index")
    
    # Create output directory if needed
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Save the index
    try:
        faiss.write_index(index, str(output_path))
        print(f"Saved FAISS index to: {output_path}")
    except Exception as e:
        print(f"Error saving FAISS index: {e}")
        return False
    
    # Validation: Test the index
    print(f"\nðŸ” VALIDATION:")
    print(f"Index dimension: {index.d}")
    print(f"Total vectors: {index.ntotal}")
    print(f"Index type: {type(index).__name__}")
    
    # Test search with first vector
    if index.ntotal > 0:
        test_vector = X[0:1]  # First vector as query
        distances, indices = index.search(test_vector, k=5)
        print(f"Test search - Top 5 similar indices: {indices[0]}")
        print(f"Test search - Distances: {distances[0]}")
        
        # Self-similarity check
        if indices[0][0] == 0 and distances[0][0] < 1e-6:
            print("âœ… Self-similarity test passed")
        else:
            print("âš ï¸ Self-similarity test failed - may indicate issues")
    
    # Feature column validation
    print(f"\nðŸ“‹ Feature Column Validation:")
    print(f"Unified columns count: {len(unified_feature_columns)}")
    print(f"CSV columns count: {len(csv_feature_columns)}")
    print(f"FAISS dimension: {index.d}")
    
    if len(unified_feature_columns) == index.d:
        print("âœ… Feature column count matches FAISS dimension")
    else:
        print("âŒ Feature column count mismatch!")
        return False
    
    return True


def validate_existing_index(csv_path: str, index_path: str):
    """Validate existing index against current CSV and unified features"""
    
    print(f"\nðŸ” VALIDATING EXISTING INDEX")
    print("="*50)
    
    # Load CSV
    try:
        df = pd.read_csv(csv_path)
        print(f"CSV loaded: {len(df)} entries")
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return False
    
    # Load index
    try:
        index = faiss.read_index(index_path)
        print(f"Index loaded: {index.ntotal} vectors, dimension {index.d}")
    except Exception as e:
        print(f"Error loading index: {e}")
        return False
    
    # Check consistency
    unified_features = [col for col in get_feature_columns() if col != 'instance_id']
    
    print(f"\nConsistency Check:")
    print(f"  CSV entries: {len(df)}")
    print(f"  Index vectors: {index.ntotal}")
    print(f"  Unified features: {len(unified_features)}")
    print(f"  Index dimension: {index.d}")
    
    # Validation results
    csv_index_match = len(df) == index.ntotal
    feature_dim_match = len(unified_features) == index.d
    
    if csv_index_match and feature_dim_match:
        print("âœ… Index is consistent with current CSV and unified features")
        return True
    else:
        print("âŒ Index inconsistency detected:")
        if not csv_index_match:
            print(f"  Entry count mismatch: CSV {len(df)} vs Index {index.ntotal}")
        if not feature_dim_match:
            print(f"  Dimension mismatch: Features {len(unified_features)} vs Index {index.d}")
        print("  Recommendation: Rebuild index")
        return False


def main():
    """Production execution: no CLI args, fixed paths resolved from repo root."""
    # Resolve repo root (src/ is under repo)
    script_path = Path(__file__).resolve()
    repo_root = script_path.parent.parent.parent

    csv_path = repo_root / "data/signatures/vuln_signatures.csv"
    index_path = repo_root / "data/signatures/faiss_structural.index"

    print("="*60)
    print("ðŸš€ BUILDING STRUCTURAL FAISS INDEX (production mode)")
    print("="*60)
    print(f"Repo root: {repo_root}")
    print(f"CSV input: {csv_path}")
    print(f"Index output: {index_path}")

    if not csv_path.exists():
        print(f"Error: CSV file not found: {csv_path}")
        print("Run src/scripts/extract_signatures.py first to generate the CSV file")
        return 2

    success = build_structural_index(str(csv_path), str(index_path))

    if success:
        df = pd.read_csv(csv_path)
        unified_features = [col for col in get_feature_columns() if col != 'instance_id']
        print(f"\nâœ… FAISS index built successfully!")
        print(f"Loaded {len(df)} signatures")
        print(f"Built FAISS index with {len(unified_features)} features")
        print(f"Index dimension: {len(unified_features)}")
        print(f"Total vectors: {len(df)}")
        print(f"Saved to: {index_path}")
        return 0
    else:
        print("\nâŒ Failed to build FAISS index")
        return 1


if __name__ == "__main__":
    main()